{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tabpfn -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install pandas -q\n",
    "!pip install matplotlib -q\n",
    "!pip install pytorch-tabnet -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, RocCurveDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import random\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_df = pd.read_csv('after_transformations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1: $7.09 - $7.72 with 12.51% of data\n",
      "Bucket 2: $7.72 - $7.88 with 12.49% of data\n",
      "Bucket 3: $7.88 - $8.01 with 13.3% of data\n",
      "Bucket 4: $8.01 - $8.10 with 11.79% of data\n",
      "Bucket 5: $8.10 - $8.22 with 13.43% of data\n",
      "Bucket 6: $8.22 - $8.38 with 11.71% of data\n",
      "Bucket 7: $8.38 - $8.74 with 12.38% of data\n",
      "Bucket 8: $8.74 - $11.74 with 12.39% of data\n"
     ]
    }
   ],
   "source": [
    "num_buckets = 6\n",
    "\n",
    "# Create quantile-based buckets with integer labels\n",
    "rent_df['rent_bucket'] = pd.qcut(rent_df['price'], q=num_buckets, labels=[i for i in range(num_buckets)])\n",
    "\n",
    "# Display the bins and distribution\n",
    "bin_edges = pd.qcut(rent_df['price'], q=num_buckets, retbins=True)[1]\n",
    "distribution = rent_df['rent_bucket'].value_counts(normalize=True, sort=False)\n",
    "\n",
    "label_to_range: dict[int, str] = {i: f\"${bin_edges[i]:,.2f} - ${bin_edges[i+1]:,.2f}\" for i in range(len(bin_edges) - 1)}\n",
    "\n",
    "for i in range(len(bin_edges) - 1):\n",
    "    print(f\"Bucket {i+1}: ${bin_edges[i]:,.2f} - ${bin_edges[i+1]:,.2f} with {round(distribution.iloc[i]*100, ndigits=2)}% of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rent_df.drop(columns=['price', 'rent_bucket'])\n",
    "y = rent_df['rent_bucket']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetClassifierWrapper(TabNetClassifier):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, *args, **kwargs):\n",
    "        return super().fit(X, y, **kwargs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return super().predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return super().predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_d': [8, 16, 32],                  # Dimension of decision prediction layer\n",
    "    'n_a': [8, 16, 32],                  # Dimension of attention embedding\n",
    "    'n_steps': [3, 5, 7],                # Number of decision steps\n",
    "    'gamma': [1.0, 1.5, 2.0],            # Feature reuse coefficient\n",
    "    'lambda_sparse': [1e-3, 1e-4, 1e-5], # Sparse regularization coefficient\n",
    "    'clip_value': [1.0, 2.0, 3.0],       # Gradient clipping value\n",
    "    'mask_type': ['sparsemax', 'entmax'], # Feature masking type\n",
    "    'seed': [42, 123, 2024]              # Random seeds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly Selected Parameters:\n",
      "{'n_d': 8, 'n_a': 8, 'n_steps': 3, 'gamma': 1.2, 'lambda_sparse': 0.001, 'optimizer_fn': None, 'seed': 42}\n",
      "epoch 0  | loss: 2.21044 | test_accuracy: 0.14982 |  0:00:01s\n",
      "epoch 1  | loss: 2.03935 | test_accuracy: 0.17061 |  0:00:01s\n",
      "epoch 2  | loss: 1.88337 | test_accuracy: 0.14265 |  0:00:02s\n",
      "epoch 3  | loss: 1.72949 | test_accuracy: 0.17491 |  0:00:02s\n",
      "epoch 4  | loss: 1.63358 | test_accuracy: 0.16487 |  0:00:03s\n",
      "epoch 5  | loss: 1.59346 | test_accuracy: 0.29606 |  0:00:03s\n",
      "epoch 6  | loss: 1.54749 | test_accuracy: 0.30036 |  0:00:04s\n",
      "epoch 7  | loss: 1.52815 | test_accuracy: 0.16774 |  0:00:04s\n",
      "epoch 8  | loss: 1.51066 | test_accuracy: 0.17133 |  0:00:05s\n",
      "epoch 9  | loss: 1.48425 | test_accuracy: 0.20502 |  0:00:05s\n",
      "epoch 10 | loss: 1.47256 | test_accuracy: 0.25663 |  0:00:06s\n",
      "epoch 11 | loss: 1.44544 | test_accuracy: 0.24373 |  0:00:06s\n",
      "epoch 12 | loss: 1.42505 | test_accuracy: 0.24086 |  0:00:07s\n",
      "epoch 13 | loss: 1.41948 | test_accuracy: 0.23584 |  0:00:07s\n",
      "epoch 14 | loss: 1.4327  | test_accuracy: 0.22437 |  0:00:08s\n",
      "epoch 15 | loss: 1.41479 | test_accuracy: 0.18566 |  0:00:09s\n",
      "epoch 16 | loss: 1.4049  | test_accuracy: 0.24516 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_test_accuracy = 0.30036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "random_params = {key: random.choice(value) for key, value in param_grid.items()}\n",
    "print(\"Randomly Selected Parameters:\")\n",
    "print(random_params)\n",
    "\n",
    "# Initialize TabNetClassifier with random parameters\n",
    "tabnet = TabNetClassifier(\n",
    "    n_d=random_params['n_d'],\n",
    "    n_a=random_params['n_a'],\n",
    "    n_steps=random_params['n_steps'],\n",
    "    gamma=random_params['gamma'],\n",
    "    lambda_sparse=random_params['lambda_sparse'],\n",
    "    seed=random_params['seed'],\n",
    "    verbose=1  # Enables detailed logging,\n",
    "    sche\n",
    ")\n",
    "\n",
    "# Train the TabNet model\n",
    "tabnet.fit(\n",
    "    X_train.values, y_train.values,\n",
    "    eval_set=[(X_test.values, y_test.values)],\n",
    "    eval_name=['test'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet = TabNetClassifier()\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=tabnet,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=3,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Predict and evaluate the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate ROC AUC (multi-class)\n",
    "y_test_binarized = label_binarize(y_test, classes=np.unique(y_test))\n",
    "roc_auc = roc_auc_score(y_test_binarized, y_proba, multi_class=\"ovr\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "for i in range(y_test_binarized.shape[1]):\n",
    "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_proba[:, i])\n",
    "    plt.plot(fpr, tpr, label=f\"Class {i}\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Guessing\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.1535623043232344, 1: 0.30386355968395157, 2: 0.24079466477302297, 3: 0.17169998416893634, 4: 0.2315410500793531, 5: 0.2843019134413132, 6: 0.6648802859498883, 7: 4.784252008243373}\n"
     ]
    }
   ],
   "source": [
    "bucket_ranges: dict[int, float] = {i: (bin_edges[i+1] - bin_edges[i]) * distribution.iloc[i] for i in range(num_buckets)}\n",
    "\n",
    "max_range = sum(bucket_ranges.values())\n",
    "normalized_bucket_ranges = {k: (log(v + 1, 1.04) / log(max_range + 1, 15))/10 for k, v in bucket_ranges.items()}\n",
    "\n",
    "print(normalized_bucket_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_penalty(t, p, bucket_ranges, bucket_range_relaxation:float, max_range:float, penalty_weight:float):\n",
    "    distance = abs(t - p) * penalty_weight\n",
    "    # Average the true and predicted bucket ranges\n",
    "    avg_range = avg_range = (bucket_ranges[t] + bucket_ranges[p]) / (bucket_range_relaxation + bucket_ranges[t] + bucket_ranges[p])\n",
    "    # Compute penalty proportional to distance and relative range\n",
    "    # The larger the distance and avg_range, the greater the penalty.\n",
    "    penalty = (distance * (avg_range / max_range))\n",
    "    return penalty\n",
    "\n",
    "def relaxed_accuracy_with_weighted_ranges(true_labels, pred_labels, bucket_ranges, min_partial_credit:float=0.08, bucket_range_relaxation:float=1.2, grace_threshold:float = 0.08, penalty_weight:float=1.5, unpack_labels:bool=False, verbose:bool=False):\n",
    "    \"\"\"\n",
    "    Calculate a \"relaxed accuracy\" that gives:\n",
    "    - Full credit (1.0) if prediction == true class.\n",
    "    - Partial credit if prediction is incorrect, with the score decreasing\n",
    "      as the class distance and bucket ranges increase.\n",
    "    - A minimum partial credit is guaranteed for any incorrect prediction \n",
    "      so that, on average, relaxed accuracy >= strict accuracy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_labels : array-like of shape (n_samples,)\n",
    "        True class labels.\n",
    "    pred_labels : array-like of shape (n_samples,)\n",
    "        Predicted class labels.\n",
    "    bucket_ranges : dict[int, float]\n",
    "        A dictionary mapping each class label to its weighted bucket range.\n",
    "        A higher float value means a larger bucket range.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean relaxed accuracy across all predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    assert 0 <= min_partial_credit <= 1, 'min_partial_credit has to between 0 and 1'\n",
    "    \n",
    "    max_range = max(bucket_ranges.values()) \n",
    "    min_partial_credit = 0.08\n",
    "\n",
    "    grace_count = 0\n",
    "    tot = 0\n",
    "    \n",
    "    scores = []\n",
    "    for t, p in zip(true_labels, pred_labels):\n",
    "        tot += 1\n",
    "        if unpack_labels:\n",
    "            p = p[0]\n",
    "\n",
    "        penalty = get_penalty(t, p, bucket_ranges, bucket_range_relaxation, max_range, penalty_weight)\n",
    "\n",
    "        score = max(1 - penalty, 0)\n",
    "        if score < grace_threshold:\n",
    "            score += min_partial_credit\n",
    "            grace_count += 1\n",
    "        \n",
    "        scores.append(score)\n",
    "    if verbose:\n",
    "        print(f\"Grace was given to {grace_count*100/tot}% of predictions.\")\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_err_field(bucket_ranges:dict[int, float]):\n",
    "    err_field = np.zeros((num_buckets, num_buckets))\n",
    "    max_range = max(bucket_ranges.values()) \n",
    "    for i in range(num_buckets):\n",
    "        for j in range(num_buckets):\n",
    "            err_field[i][j] = get_penalty(i, j, bucket_ranges, 300, max_range=max_range, penalty_weight=100)\n",
    "    return err_field\n",
    "\n",
    "err_field = calculate_err_field(normalized_bucket_ranges)\n",
    "err_field_tensor = torch.from_numpy(err_field).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss_fn(y_pred, y_true):\n",
    "    softmax_pred = torch.nn.Softmax(dim=-1)(y_pred)  # Compute softmax on GPU\n",
    "    pred = torch.argmax(softmax_pred, dim=1)         # Compute argmax on GPU\n",
    "\n",
    "    # Perform the lookup directly on the GPU\n",
    "    loss = err_field_tensor[y_true, pred]\n",
    "\n",
    "    # Return the sum of the resulting tensor\n",
    "    return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 80.21245| test_accuracy: 0.14194 |  0:00:03s\n",
      "epoch 1  | loss: 82.70682| test_accuracy: 0.13118 |  0:00:06s\n",
      "epoch 2  | loss: 81.70358| test_accuracy: 0.12832 |  0:00:10s\n",
      "epoch 3  | loss: 85.02529| test_accuracy: 0.11828 |  0:00:13s\n",
      "epoch 4  | loss: 85.9856 | test_accuracy: 0.14265 |  0:00:17s\n",
      "epoch 5  | loss: 80.8981 | test_accuracy: 0.11039 |  0:00:20s\n",
      "epoch 6  | loss: 79.87609| test_accuracy: 0.12832 |  0:00:24s\n",
      "epoch 7  | loss: 74.3758 | test_accuracy: 0.11828 |  0:00:27s\n",
      "epoch 8  | loss: 74.69125| test_accuracy: 0.1147  |  0:00:31s\n",
      "epoch 9  | loss: 77.6974 | test_accuracy: 0.1319  |  0:00:34s\n",
      "epoch 10 | loss: 77.4005 | test_accuracy: 0.1233  |  0:00:37s\n",
      "epoch 11 | loss: 79.32454| test_accuracy: 0.11613 |  0:00:41s\n",
      "epoch 12 | loss: 79.79361| test_accuracy: 0.119   |  0:00:44s\n",
      "epoch 13 | loss: 78.5486 | test_accuracy: 0.14624 |  0:00:48s\n",
      "epoch 14 | loss: 78.68648| test_accuracy: 0.12473 |  0:00:51s\n",
      "epoch 15 | loss: 77.26858| test_accuracy: 0.119   |  0:00:55s\n",
      "epoch 16 | loss: 77.26437| test_accuracy: 0.12975 |  0:00:58s\n",
      "epoch 17 | loss: 79.33923| test_accuracy: 0.1319  |  0:01:01s\n",
      "epoch 18 | loss: 79.45085| test_accuracy: 0.13763 |  0:01:05s\n",
      "epoch 19 | loss: 76.30684| test_accuracy: 0.1362  |  0:01:08s\n",
      "epoch 20 | loss: 73.90117| test_accuracy: 0.11541 |  0:01:12s\n",
      "epoch 21 | loss: 74.36614| test_accuracy: 0.11111 |  0:01:15s\n",
      "epoch 22 | loss: 74.67706| test_accuracy: 0.10036 |  0:01:18s\n",
      "epoch 23 | loss: 75.33312| test_accuracy: 0.10681 |  0:01:22s\n",
      "epoch 24 | loss: 72.17394| test_accuracy: 0.10681 |  0:01:25s\n",
      "epoch 25 | loss: 74.35389| test_accuracy: 0.10681 |  0:01:29s\n",
      "epoch 26 | loss: 72.39337| test_accuracy: 0.10179 |  0:01:32s\n",
      "epoch 27 | loss: 72.4902 | test_accuracy: 0.10323 |  0:01:36s\n",
      "epoch 28 | loss: 71.03769| test_accuracy: 0.12043 |  0:01:39s\n",
      "epoch 29 | loss: 71.1732 | test_accuracy: 0.1147  |  0:01:43s\n",
      "epoch 30 | loss: 70.50514| test_accuracy: 0.10681 |  0:01:46s\n",
      "epoch 31 | loss: 71.8166 | test_accuracy: 0.11183 |  0:01:49s\n",
      "epoch 32 | loss: 71.37354| test_accuracy: 0.12186 |  0:01:53s\n",
      "epoch 33 | loss: 73.8489 | test_accuracy: 0.11183 |  0:01:56s\n",
      "epoch 34 | loss: 73.79411| test_accuracy: 0.12473 |  0:02:00s\n",
      "epoch 35 | loss: 74.3805 | test_accuracy: 0.12473 |  0:02:03s\n",
      "epoch 36 | loss: 72.56553| test_accuracy: 0.12043 |  0:02:06s\n",
      "epoch 37 | loss: 74.12297| test_accuracy: 0.11971 |  0:02:10s\n",
      "epoch 38 | loss: 72.45829| test_accuracy: 0.12401 |  0:02:13s\n",
      "epoch 39 | loss: 72.9088 | test_accuracy: 0.119   |  0:02:17s\n",
      "epoch 40 | loss: 70.67871| test_accuracy: 0.12545 |  0:02:20s\n",
      "epoch 41 | loss: 71.54283| test_accuracy: 0.11971 |  0:02:24s\n",
      "epoch 42 | loss: 72.22944| test_accuracy: 0.12401 |  0:02:27s\n",
      "epoch 43 | loss: 70.79987| test_accuracy: 0.11541 |  0:02:30s\n",
      "epoch 44 | loss: 72.67288| test_accuracy: 0.11398 |  0:02:34s\n",
      "epoch 45 | loss: 70.63421| test_accuracy: 0.1147  |  0:02:37s\n",
      "epoch 46 | loss: 72.31294| test_accuracy: 0.11541 |  0:02:41s\n",
      "epoch 47 | loss: 71.38442| test_accuracy: 0.10968 |  0:02:44s\n",
      "epoch 48 | loss: 71.32312| test_accuracy: 0.11613 |  0:02:47s\n",
      "epoch 49 | loss: 73.34739| test_accuracy: 0.11613 |  0:02:51s\n",
      "epoch 50 | loss: 73.08287| test_accuracy: 0.1233  |  0:02:54s\n",
      "epoch 51 | loss: 72.38227| test_accuracy: 0.11541 |  0:02:58s\n",
      "epoch 52 | loss: 71.64771| test_accuracy: 0.1276  |  0:03:01s\n",
      "epoch 53 | loss: 71.2827 | test_accuracy: 0.1319  |  0:03:05s\n",
      "epoch 54 | loss: 72.5774 | test_accuracy: 0.12832 |  0:03:08s\n",
      "epoch 55 | loss: 71.07172| test_accuracy: 0.11971 |  0:03:12s\n",
      "epoch 56 | loss: 72.6953 | test_accuracy: 0.12258 |  0:03:15s\n",
      "epoch 57 | loss: 73.32694| test_accuracy: 0.11685 |  0:03:19s\n",
      "epoch 58 | loss: 72.8406 | test_accuracy: 0.1233  |  0:03:22s\n",
      "epoch 59 | loss: 72.7883 | test_accuracy: 0.1233  |  0:03:25s\n",
      "epoch 60 | loss: 73.23506| test_accuracy: 0.12258 |  0:03:29s\n",
      "epoch 61 | loss: 74.25598| test_accuracy: 0.12473 |  0:03:32s\n",
      "epoch 62 | loss: 72.42803| test_accuracy: 0.1276  |  0:03:36s\n",
      "epoch 63 | loss: 72.39182| test_accuracy: 0.12616 |  0:03:39s\n",
      "epoch 64 | loss: 73.44896| test_accuracy: 0.1276  |  0:03:42s\n",
      "epoch 65 | loss: 72.01992| test_accuracy: 0.12545 |  0:03:46s\n",
      "epoch 66 | loss: 73.21186| test_accuracy: 0.11756 |  0:03:49s\n",
      "epoch 67 | loss: 73.22525| test_accuracy: 0.12043 |  0:03:53s\n",
      "epoch 68 | loss: 71.90333| test_accuracy: 0.12115 |  0:03:56s\n",
      "epoch 69 | loss: 73.28451| test_accuracy: 0.11613 |  0:04:00s\n",
      "epoch 70 | loss: 73.18932| test_accuracy: 0.11541 |  0:04:03s\n",
      "epoch 71 | loss: 72.15145| test_accuracy: 0.119   |  0:04:06s\n",
      "epoch 72 | loss: 72.93592| test_accuracy: 0.12186 |  0:04:10s\n",
      "epoch 73 | loss: 72.82145| test_accuracy: 0.12975 |  0:04:13s\n",
      "epoch 74 | loss: 72.94581| test_accuracy: 0.12688 |  0:04:17s\n",
      "epoch 75 | loss: 72.4888 | test_accuracy: 0.119   |  0:04:20s\n",
      "epoch 76 | loss: 73.10317| test_accuracy: 0.12473 |  0:04:24s\n",
      "epoch 77 | loss: 74.48858| test_accuracy: 0.119   |  0:04:27s\n",
      "epoch 78 | loss: 73.02766| test_accuracy: 0.12186 |  0:04:30s\n",
      "epoch 79 | loss: 73.17188| test_accuracy: 0.11685 |  0:04:34s\n",
      "epoch 80 | loss: 74.22209| test_accuracy: 0.12186 |  0:04:37s\n",
      "epoch 81 | loss: 73.01895| test_accuracy: 0.119   |  0:04:41s\n",
      "epoch 82 | loss: 72.53841| test_accuracy: 0.12043 |  0:04:44s\n",
      "epoch 83 | loss: 72.96231| test_accuracy: 0.12115 |  0:04:48s\n",
      "epoch 84 | loss: 74.664  | test_accuracy: 0.12186 |  0:04:51s\n",
      "epoch 85 | loss: 71.98476| test_accuracy: 0.119   |  0:04:55s\n",
      "epoch 86 | loss: 73.07957| test_accuracy: 0.11613 |  0:04:58s\n",
      "epoch 87 | loss: 73.07834| test_accuracy: 0.1147  |  0:05:01s\n",
      "epoch 88 | loss: 73.70557| test_accuracy: 0.11398 |  0:05:05s\n",
      "epoch 89 | loss: 72.4865 | test_accuracy: 0.11828 |  0:05:08s\n",
      "epoch 90 | loss: 73.41778| test_accuracy: 0.12258 |  0:05:12s\n",
      "epoch 91 | loss: 73.39767| test_accuracy: 0.12186 |  0:05:15s\n",
      "epoch 92 | loss: 75.86422| test_accuracy: 0.12616 |  0:05:18s\n",
      "epoch 93 | loss: 74.35621| test_accuracy: 0.12616 |  0:05:22s\n",
      "epoch 94 | loss: 71.88644| test_accuracy: 0.12186 |  0:05:25s\n",
      "epoch 95 | loss: 72.42299| test_accuracy: 0.11685 |  0:05:29s\n",
      "epoch 96 | loss: 72.96338| test_accuracy: 0.12115 |  0:05:32s\n",
      "epoch 97 | loss: 72.29027| test_accuracy: 0.11971 |  0:05:36s\n",
      "epoch 98 | loss: 74.72269| test_accuracy: 0.12832 |  0:05:39s\n",
      "epoch 99 | loss: 73.02032| test_accuracy: 0.12043 |  0:05:42s\n",
      "epoch 100| loss: 73.43779| test_accuracy: 0.12258 |  0:05:46s\n",
      "epoch 101| loss: 72.52731| test_accuracy: 0.119   |  0:05:50s\n",
      "epoch 102| loss: 71.483  | test_accuracy: 0.11756 |  0:05:53s\n",
      "epoch 103| loss: 73.19106| test_accuracy: 0.12186 |  0:05:56s\n",
      "epoch 104| loss: 73.70798| test_accuracy: 0.11971 |  0:06:00s\n",
      "epoch 105| loss: 73.28151| test_accuracy: 0.12115 |  0:06:03s\n",
      "epoch 106| loss: 72.9023 | test_accuracy: 0.12616 |  0:06:07s\n",
      "epoch 107| loss: 73.75488| test_accuracy: 0.1233  |  0:06:10s\n",
      "epoch 108| loss: 73.34504| test_accuracy: 0.12401 |  0:06:13s\n",
      "epoch 109| loss: 72.49838| test_accuracy: 0.1233  |  0:06:17s\n",
      "epoch 110| loss: 73.97961| test_accuracy: 0.12115 |  0:06:20s\n",
      "epoch 111| loss: 73.14326| test_accuracy: 0.11971 |  0:06:24s\n",
      "epoch 112| loss: 72.40528| test_accuracy: 0.1233  |  0:06:27s\n",
      "epoch 113| loss: 72.66191| test_accuracy: 0.12258 |  0:06:30s\n",
      "\n",
      "Early stopping occurred at epoch 113 with best_epoch = 13 and best_test_accuracy = 0.14624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "tabnet = TabNetClassifier(\n",
    "    optimizer_fn=AdamW,  # Use AdamW optimizer\n",
    "    optimizer_params=dict(lr=0.01, weight_decay=1e-2),  # Specify AdamW parameters\n",
    "    n_d=128,\n",
    "    n_a=128,\n",
    "    n_steps=32,\n",
    "    gamma=1.5,\n",
    "    lambda_sparse=1e-3,\n",
    "    mask_type=\"sparsemax\",\n",
    "    device_name='cuda',\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,  # Scheduler function\n",
    "    scheduler_params={\n",
    "        'mode': 'min',        # Minimize the monitored metric\n",
    "        'factor': 0.8,        # Reduce learning rate by a factor of 0.1\n",
    "        'patience': 5,        # Number of epochs with no improvement after which learning rate will be reduced\n",
    "        'verbose': True       # Print a message when the learning rate is reduced\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "tabnet.fit(\n",
    "    X_train.values, y_train.values,\n",
    "    eval_set=[(X_test.values, y_test.values)],\n",
    "    eval_name=['test'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=1000,\n",
    "    loss_fn=my_loss_fn,\n",
    "    patience=100,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at tabnet_model.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tabnet_model.zip'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabnet.save_model(\"tabnet_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
